{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06eb1bf3-ab2e-48c7-9c40-6197a6f63382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, handlers=[logging.StreamHandler()])\n",
    "logger = logging.getLogger(\"mdp_prop\")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20445884-4c57-4c2b-859c-7212795c56f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Config\n",
    "\n",
    "Prerequisite is to allow the Databrick environment to load from ADLS container. Follow this tutorial:\n",
    "\n",
    "[Tutorial: Connect to Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-gb/azure/databricks/connect/storage/tutorial-azure-storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae859618-e81a-433c-83b5-104b2bf85828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set an Entra ID for the Databricks workspace\n",
    "DBK_SECRET_SCOPE = \"tichack2024kv\" # Databricks secret scope to access Azure Key Vault\n",
    "AKV_KEY_NAME = \"analytical-databricks-key\" # Azure Key Vault\n",
    "ENTRA_APP_ID = \"9156dfe1-254b-4047-9f1a-a8fd3e79787d\" # Entra ID for the Databricks workspace\n",
    "ENTRA_DIRECTORY_ID = \"565f1c8e-754e-473e-8352-ac5b86a38c93\" # Tenant ID of Entra App\n",
    "\n",
    "# ADLS container with the data\n",
    "STORAGE_ACC = \"agenticaiamlws\" # Storage Account\n",
    "ADLS_CONTAINER = \"azureml-blobstore-03a975f6-17cd-4334-a581-d30d363b62ab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b93c279-5332-490b-b02c-7be2e702a6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_credential = dbutils.secrets.get(scope=DBK_SECRET_SCOPE, key=AKV_KEY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3a3633-ecb6-441a-b722-0ebc3bd8900d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extract MDP Zip\n",
    "\n",
    "[Mounting cloud object storage on Azure Databricks](https://learn.microsoft.com/en-gb/azure/databricks/dbfs/mounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f30b44de-ca33-46db-97c2-9d812612b035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Already mounted to /mnt/adls/ No need to mount twice\n",
    "# configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "#           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "#           \"fs.azure.account.oauth2.client.id\": ENTRA_APP_ID,\n",
    "#           \"fs.azure.account.oauth2.client.secret\": service_credential,\n",
    "#           \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{ENTRA_DIRECTORY_ID}/oauth2/token\"}\n",
    "\n",
    "# dbutils.fs.mount(\n",
    "#     source=f\"abfss://{ADLS_CONTAINER}@{STORAGE_ACC}.dfs.core.windows.net\",\n",
    "#     mount_point=\"/mnt/adls\",\n",
    "#     extra_configs=configs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d85710-07a6-4da4-a1e2-408d3aebc09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# Define Paths\n",
    "adls_mnt_path = \"/dbfs/mnt/adls_2\"\n",
    "\n",
    "# # Fake path\n",
    "# zip_adls_file = \"<fake/path>\" # Challenge path\n",
    "# extract_path = \"<fake/path>\"\n",
    "\n",
    "# # Challenge\n",
    "# zip_adls_file = \"million_playlist_dataset/spotify_million_playlist_dataset_challenge.zip\" # Challenge path\n",
    "# extract_path = \"/dbfs/mnt/adls/challenge_dataset/\"\n",
    "\n",
    "# # MLD\n",
    "# zip_adls_file = \"million_playlist_dataset/spotify_million_playlist_dataset.zip\" # MPD path\n",
    "# extract_path = \"/dbfs/mnt/adls/mld_dataset/\"\n",
    "\n",
    "# # Create extract folder\n",
    "# zip_path = os.path.join(adls_mnt_path, zip_adls_file)  # Databricks paths use /dbfs/\n",
    "# os.makedirs(extract_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f938dbb7-121e-4c6b-b01d-a881e34a691a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The extraction can take 30 minutes. Total extracted JSON size ~33GB and 1000 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4313b863-0644-4a2e-afac-a0b1d320cfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Extract ZIP file\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "#     zf.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a50f940-a19e-4583-bb29-a1d9c3f657e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Flatten MPD JSONs\n",
    "\n",
    "Convert MPD json to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad2f97c-bf93-4dfc-a96c-cba5cbf9c9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set spark connection to ADLS\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{STORAGE_ACC}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{STORAGE_ACC}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{STORAGE_ACC}.dfs.core.windows.net\", ENTRA_APP_ID)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{STORAGE_ACC}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{STORAGE_ACC}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{ENTRA_DIRECTORY_ID}/oauth2/token\")\n",
    "\n",
    "# # Spark configurations (set in the Databricks compute configuration)\n",
    "# spark.conf.set(\"spark.databricks.cluster.profile\", \"singleNode\")\n",
    "# spark.conf.set(\"spark.master\", \"local[*, 4]\")\n",
    "# spark.conf.set(\"spark.driver.maxResultSize\", \"8g\")\n",
    "\n",
    "# Test spark connection\n",
    "df_titanic = spark.read.csv(f\"abfss://{ADLS_CONTAINER}@{STORAGE_ACC}.dfs.core.windows.net/titanic.csv\", header=True)\n",
    "display(df_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da492dcd-d910-4a63-98b9-09ea8e87bbb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Million Playlist Dataset paths\n",
    "mdp_adls_path = f\"abfss://{ADLS_CONTAINER}@{STORAGE_ACC}.dfs.core.windows.net/\" # adls path\n",
    "mpd_json_datastore = \"mld_dataset/data/\" # mpd json path\n",
    "\n",
    "# List all files in the ADLS\n",
    "logger.info(\"List JSON files as ADLS path.\")\n",
    "mdp_json_paths = []\n",
    "for file in dbutils.fs.ls(os.path.join(mdp_adls_path, mpd_json_datastore)):\n",
    "    mdp_json_paths.append(file.path)\n",
    "\n",
    "# List all files in local mount\n",
    "logger.info(\"List JSON files as local path.\")\n",
    "mdp_json_local = \"mld_dataset/data\"\n",
    "mdp_json_paths_local = []\n",
    "for file in dbutils.fs.ls(f\"/mnt/adls_2/{mdp_json_local}\"):\n",
    "    mdp_json_paths_local.append(file.path)\n",
    "\n",
    "mdp_json_paths_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0a19dc-d22b-4c69-b2a6-0ba83de8c16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Convert JSON to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1623c1d9-9257-44a0-a2aa-920a30a8d974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any\n",
    "\n",
    "def flatten_mpd_json(json_paths: list[str], parquet_path: str) -> None:\n",
    "    for fpath in tqdm(json_paths):\n",
    "        fpath = re.sub(\"dbfs:\", \"/dbfs\", fpath) # Convert to Databricks path\n",
    "        with open(fpath, 'r') as fi:\n",
    "            data = json.load(fi)\n",
    "        if 'playlists' in data:\n",
    "            playlists = data['playlists']\n",
    "            df_playlists = (\n",
    "                spark.createDataFrame(playlists)\n",
    "                .withColumn(\"track\", sf.explode(\"tracks\"))    \n",
    "                .drop(\"tracks\")\n",
    "            )\n",
    "            # Extract track attributes as column\n",
    "            playlist_columns = df_playlists.columns\n",
    "            track_columns = [\n",
    "                sf.col(\"track\").getItem(k).alias(\n",
    "                    \"track_\" + k if k not in ['track_name', 'track_uri'] else k\n",
    "                )\n",
    "                for k in [\n",
    "                \"pos\", \"artist_name\", \"track_uri\", \"artist_uri\", \n",
    "                \"track_name\", \"album_uri\", \"duration_ms\", \"album_name\"\n",
    "            ]]\n",
    "            df_playlists = df_playlists.select(*(playlist_columns + track_columns)).drop(\"track\")\n",
    "            # Save to parquet\n",
    "            fname_parts = os.path.basename(fpath).split(\".\")[:-1]\n",
    "            fname_parts.append('parquet')\n",
    "            fname = '.'.join(fname_parts)\n",
    "            (\n",
    "                df_playlists\n",
    "                .coalesce(1)\n",
    "                .write\n",
    "                .mode(\"overwrite\")\n",
    "                .format(\"parquet\")\n",
    "                .save(os.path.join(parquet_path, fname))\n",
    "            )\n",
    "        else:\n",
    "            playlist = []\n",
    "            logger.error(f\"No playlists in {fpath}\")\n",
    "\n",
    "\n",
    "# # Convert MDP json to parquet and save to ADLS\n",
    "mpd_parquet_raw = \"mpd_parquet_raw\"\n",
    "mdp_parquet_raw_path = os.path.join(adls_mnt_path, mpd_parquet_raw)\n",
    "logging.info(\"Saving MDP json to %s\", mdp_parquet_raw_path)\n",
    "# os.makedirs(os.path.join(adls_mnt_path, mpd_parquet_raw), exist_ok=True)\n",
    "# flatten_mpd_json(\n",
    "#     json_paths = mdp_json_paths_local,\n",
    "#     parquet_path = os.path.join(mdp_adls_path, mpd_parquet_raw)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762e732c-ee3a-4e20-9a62-c60f5613e40b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load from ADLS\n",
    "df_playlists_raw = spark.read.parquet(os.path.join(mdp_adls_path, mpd_parquet_raw, \"*.parquet\"))\n",
    "print(df_playlists_raw.count())\n",
    "display(df_playlists_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d60b7f-62d2-4d4b-817c-b5a78cde3693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Load Track Features\n",
    "\n",
    "Load track features from Spotify API.\n",
    "\n",
    "Note: the remote ADLS container is sometimes mounted to /dbfs/dbfs/mnt/adls_2 and sometimes mounted to /dbfs/mnt/adls_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f167ec2-9ae1-4cc5-a94e-f05b510730ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Write unique track and artist ID table to ADLS\n",
    "# df_unique_tracks = df_playlists_raw.groupby(\"track_uri\", \"track_album_uri\", \"track_artist_uri\").agg(sf.count(\"track_pos\").alias(\"freq\"))\n",
    "# mpd_unique_tracks_file = os.path.join(\"/mnt/adls_2\", \"unique_tracks.parquet\") \n",
    "# print(df_unique_tracks.count())\n",
    "# df_unique_tracks.coalesce(1).write.mode(\"overwrite\").format(\"parquet\").save(mpd_unique_tracks_file)\n",
    "\n",
    "# Read unique track and artist ID table back\n",
    "fpath = os.path.join(mdp_adls_path, \"unique_tracks.parquet\")\n",
    "logger.info(\"Load unique tracks from %s\", fpath)\n",
    "df_unique_tracks = spark.read.parquet(fpath)\n",
    "\n",
    "display(df_unique_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db77755d-fa6b-4cdc-a8a7-a4a6ade72a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load track and artist features from Spotify API\n",
    "import re\n",
    "import time\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional, Sequence\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "class SpotifyFeatureLoader:\n",
    "    \"\"\"\n",
    "    Load track features from spotify.\n",
    "    \"\"\"\n",
    "    # Declare hints\n",
    "    client_cred: SpotifyClientCredentials\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client_id: Optional[str] = os.getenv(\"SPOTIFY_CLIENT_ID\"),\n",
    "        client_secret: Optional[str] = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n",
    "    ):\n",
    "        # Connect to Spotify App\n",
    "        self.client_cred = SpotifyClientCredentials(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret\n",
    "        )\n",
    "\n",
    "    def load_spotify_track_features(\n",
    "        self, \n",
    "        uri: Sequence[str],\n",
    "        batch_size: Optional[int] = 100,\n",
    "        sleep_time: Optional[int] = None\n",
    "    ):\n",
    "        chunks = [uri[i:i+batch_size] for i in range(0, len(uri), batch_size)]\n",
    "        chunks_df_list = []\n",
    "        for chunk in tqdm(chunks):\n",
    "            df_tracks = self._load_spotify_track_features(chunk)\n",
    "            chunks_df_list.append(df_tracks)\n",
    "            if sleep_time:\n",
    "                time.sleep(sleep_time)\n",
    "        return pd.concat(chunks_df_list, ignore_index=True)\n",
    "\n",
    "    def _load_spotify_track_features(\n",
    "        self,\n",
    "        uri: Sequence[str],\n",
    "        \n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Note: Audio features endpoint is deprecated by spotify.\n",
    "        Notification: [Introducing some changes to our Web API](https://developer.spotify.com/blog/2024-11-27-changes-to-the-web-api)\n",
    "        Forum discussion: [Changes to Web API](https://community.spotify.com/t5/Spotify-for-Developers/Changes-to-Web-API/td-p/6540414)\n",
    "        \"\"\"\n",
    "        sp = spotipy.Spotify(\n",
    "            client_credentials_manager=self.client_cred,\n",
    "            requests_timeout=10,\n",
    "            retries=10\n",
    "        )\n",
    "        # Get track attributes\n",
    "        uri_stripped = [re.sub(\"spotify:track:\", \"\", u) for u in uri]\n",
    "        tracks_json = sp.tracks(uri_stripped)\n",
    "        # # Get track audio features (Audio feature endpoints is deprecated by Spotify)\n",
    "        # time.sleep(1) # Sleep 20 milliseconds before next call\n",
    "        # audio_features = sp.audio_features(uri_stripped)\n",
    "        df_tracks = pd.json_normalize(tracks_json[\"tracks\"])[[\n",
    "            'popularity', 'href', 'name', 'uri', 'duration_ms'\n",
    "        ]]\n",
    "        return df_tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e43e1cc-4f70-4f37-b48d-c6f3db3bf5c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_tracks_uri = df_unique_tracks[['track_uri']].toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b3d089a-1434-48cf-b708-2b18d3d64372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_tracks_uri[:200]['track_uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b9a018-7b10-4af1-b214-d48da3aa8364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sfl = SpotifyFeatureLoader()\n",
    "test_track_uri = [\"spotify:track:3oLXQgbcC34Oo4gFHCeTmi\", \"spotify:track:6TRp2628QKH3kY6KrCnjqp\"]\n",
    "df_test_tracks = sfl.load_spotify_track_features(pd_tracks_uri[:2000]['track_uri'].to_list(), 100)\n",
    "df_test_tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6893f3-a15b-4220-b8a1-835b9c15b68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db60ecf8-6d5e-4c75-b9c0-9b7c2656615a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.json_normalize(tracks_json[\"tracks\"])[['popularity', 'href', 'name', 'uri', 'duration_ms']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6033a3-4eea-4b25-87a6-84218f39e83e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tracks_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da062fd2-bef0-4fce-8125-604125b5a9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Get unique artist table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fefb902-c382-44a1-8fd7-72138b5e72da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4ae403-6ad3-4a7b-bd0b-ffcd2441bc85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['playlists'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42593cb1-b330-4657-9989-da57ecafbe54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load MDP. About 6 minutes\n",
    "df = spark.read.option(\"multiline\", \"true\").json(mdp_json_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9e41229-69d8-4461-99e6-604aa7b756e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "```\n",
    "+--------------------+--------------------+\n",
    "|                info|           playlists|\n",
    "+--------------------+--------------------+\n",
    "|{2017-12-03 08:41...|[{false, NULL, 11...|\n",
    "+--------------------+--------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49be0607-80ed-49b2-9ada-3fe16db8ac47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df[[\"playlists\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47e2d69e-007c-42b8-b03d-a3d3b40f311e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "An example playlist entry.\n",
    "\n",
    "```\n",
    "{\n",
    "        \"name\": \"musical\",\n",
    "        \"collaborative\": \"false\",\n",
    "        \"pid\": 5,\n",
    "        \"modified_at\": 1493424000,\n",
    "        \"num_albums\": 7,\n",
    "        \"num_tracks\": 12,\n",
    "        \"num_followers\": 1,\n",
    "        \"num_edits\": 2,\n",
    "        \"duration_ms\": 2657366,\n",
    "        \"num_artists\": 6,\n",
    "        \"tracks\": [\n",
    "            {\n",
    "                \"pos\": 0,\n",
    "                \"artist_name\": \"Degiheugi\",\n",
    "                \"track_uri\": \"spotify:track:7vqa3sDmtEaVJ2gcvxtRID\",\n",
    "                \"artist_uri\": \"spotify:artist:3V2paBXEoZIAhfZRJmo2jL\",\n",
    "                \"track_name\": \"Finalement\",\n",
    "                \"album_uri\": \"spotify:album:2KrRMJ9z7Xjoz1Az4O6UML\",\n",
    "                \"duration_ms\": 166264,\n",
    "                \"album_name\": \"Dancing Chords and Fireflies\"\n",
    "            },\n",
    "            {\n",
    "                \"pos\": 1,\n",
    "                \"artist_name\": \"Degiheugi\",\n",
    "                \"track_uri\": \"spotify:track:23EOmJivOZ88WJPUbIPjh6\",\n",
    "                \"artist_uri\": \"spotify:artist:3V2paBXEoZIAhfZRJmo2jL\",\n",
    "                \"track_name\": \"Betty\",\n",
    "                \"album_uri\": \"spotify:album:3lUSlvjUoHNA8IkNTqURqd\",\n",
    "                \"duration_ms\": 235534,\n",
    "                \"album_name\": \"Endless Smile\"\n",
    "            },\n",
    "            {\n",
    "                \"pos\": 2,\n",
    "                \"artist_name\": \"Degiheugi\",\n",
    "                \"track_uri\": \"spotify:track:1vaffTCJxkyqeJY7zF9a55\",\n",
    "                \"artist_uri\": \"spotify:artist:3V2paBXEoZIAhfZRJmo2jL\",\n",
    "                \"track_name\": \"Some Beat in My Head\",\n",
    "                \"album_uri\": \"spotify:album:2KrRMJ9z7Xjoz1Az4O6UML\",\n",
    "                \"duration_ms\": 268050,\n",
    "                \"album_name\": \"Dancing Chords and Fireflies\"\n",
    "            },\n",
    "            // 8 tracks omitted\n",
    "            {\n",
    "                \"pos\": 11,\n",
    "                \"artist_name\": \"Mo' Horizons\",\n",
    "                \"track_uri\": \"spotify:track:7iwx00eBzeSSSy6xfESyWN\",\n",
    "                \"artist_uri\": \"spotify:artist:3tuX54dqgS8LsGUvNzgrpP\",\n",
    "                \"track_name\": \"Fever 99\\u00b0\",\n",
    "                \"album_uri\": \"spotify:album:2Fg1t2tyOSGWkVYHlFfXVf\",\n",
    "                \"duration_ms\": 364320,\n",
    "                \"album_name\": \"Come Touch The Sun\"\n",
    "            }\n",
    "        ],\n",
    "\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91fd1067-ca12-4171-a8b2-b0d376bd6252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mpd_data_process",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
