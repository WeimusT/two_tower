{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06eb1bf3-ab2e-48c7-9c40-6197a6f63382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR, handlers=[logging.StreamHandler()])\n",
    "logger = logging.getLogger(\"mdp_prop\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aec21fee-1adf-4106-9336-35a7fb4c5f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20445884-4c57-4c2b-859c-7212795c56f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Config\n",
    "\n",
    "Prerequisite is to allow the Databrick environment to load from ADLS container. Follow this tutorial:\n",
    "\n",
    "[Tutorial: Connect to Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-gb/azure/databricks/connect/storage/tutorial-azure-storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae859618-e81a-433c-83b5-104b2bf85828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBK_SECRET_SCOPE = \"tichack2024kv\" # Databricks secret scope to access Azure Key Vault\n",
    "AKV_KEY_NAME = \"analytical-databricks-key\" # Azure Key Vault\n",
    "ENTRA_APP_ID = \"9156dfe1-254b-4047-9f1a-a8fd3e79787d\"\n",
    "ENTRA_DIRECTORY_ID = \"565f1c8e-754e-473e-8352-ac5b86a38c93\" # Tenant ID of Entra App\n",
    "\n",
    "STORAGE_ACC = \"agenticaiamlws\" # Storage Account\n",
    "ADLS_CONTAINER = \"azureml-blobstore-03a975f6-17cd-4334-a581-d30d363b62ab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b93c279-5332-490b-b02c-7be2e702a6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_credential = dbutils.secrets.get(scope=DBK_SECRET_SCOPE, key=AKV_KEY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3a3633-ecb6-441a-b722-0ebc3bd8900d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extract MDP Zip\n",
    "\n",
    "[Mounting cloud object storage on Azure Databricks](https://learn.microsoft.com/en-gb/azure/databricks/dbfs/mounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f30b44de-ca33-46db-97c2-9d812612b035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Already mounted to /mnt/adls/ No need to mount twice\n",
    "# configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "#           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "#           \"fs.azure.account.oauth2.client.id\": ENTRA_APP_ID,\n",
    "#           \"fs.azure.account.oauth2.client.secret\": service_credential,\n",
    "#           \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{ENTRA_DIRECTORY_ID}/oauth2/token\"}\n",
    "\n",
    "# dbutils.fs.mount(\n",
    "#     source=f\"abfss://{ADLS_CONTAINER}@{STORAGE_ACC}.dfs.core.windows.net\",\n",
    "#     mount_point=\"/mnt/adls\",\n",
    "#     extra_configs=configs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d85710-07a6-4da4-a1e2-408d3aebc09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# Define Paths\n",
    "adls_mnt_path = \"/dbfs/mnt/adls\"\n",
    "\n",
    "# # Fake path\n",
    "# zip_adls_file = \"<fake/path>\" # Challenge path\n",
    "# extract_path = \"<fake/path>\"\n",
    "\n",
    "# # Challenge\n",
    "# zip_adls_file = \"million_playlist_dataset/spotify_million_playlist_dataset_challenge.zip\" # Challenge path\n",
    "# extract_path = \"/dbfs/mnt/adls/challenge_dataset/\"\n",
    "\n",
    "# # MLD\n",
    "# zip_adls_file = \"million_playlist_dataset/spotify_million_playlist_dataset.zip\" # MPD path\n",
    "# extract_path = \"/dbfs/mnt/adls/mld_dataset/\"\n",
    "\n",
    "# # Create extract folder\n",
    "# zip_path = os.path.join(adls_mnt_path, zip_adls_file)  # Databricks paths use /dbfs/\n",
    "# os.makedirs(extract_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f938dbb7-121e-4c6b-b01d-a881e34a691a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The extraction can take 30 minutes. Total extracted JSON size ~33GB and 1000 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4313b863-0644-4a2e-afac-a0b1d320cfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Extract ZIP file\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "#     zf.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a50f940-a19e-4583-bb29-a1d9c3f657e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Flatten MPD JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad2f97c-bf93-4dfc-a96c-cba5cbf9c9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set spark connection to ADLS\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{STORAGE_ACC}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{STORAGE_ACC}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{STORAGE_ACC}.dfs.core.windows.net\", ENTRA_APP_ID)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{STORAGE_ACC}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{STORAGE_ACC}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{ENTRA_DIRECTORY_ID}/oauth2/token\")\n",
    "\n",
    "# Spark configurations\n",
    "\n",
    "\n",
    "# Test spark connection\n",
    "df_titanic = spark.read.csv(f\"abfss://{ADLS_CONTAINER}@{STORAGE_ACC}.dfs.core.windows.net/titanic.csv\", header=True)\n",
    "display(df_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da492dcd-d910-4a63-98b9-09ea8e87bbb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Million Playlist Dataset path\n",
    "mpd_json_datastore = \"mld_dataset/data/\" # mdp json path\n",
    "mpd_dataset = \"mdp_dataset\" # mdp parquet path\n",
    "os.makedirs(os.path.join(adls_mnt_path, mpd_dataset), exist_ok=True)\n",
    "\n",
    "# List all files in the ADLS\n",
    "logger.info(\"List JSON files as ADLS path.\")\n",
    "mdp_adls_path = f\"abfss://{ADLS_CONTAINER}@{STORAGE_ACC}.dfs.core.windows.net/{mpd_json_datastore}\" # adls path\n",
    "mdp_json_paths = []\n",
    "for file in dbutils.fs.ls(mdp_adls_path):\n",
    "    mdp_json_paths.append(file.path)\n",
    "\n",
    "# List all files in local mount\n",
    "logger.info(\"List JSON files as local path.\")\n",
    "mdp_json_local = \"mld_dataset/data\"\n",
    "mdp_json_paths_local = []\n",
    "for file in dbutils.fs.ls(f\"/mnt/adls/{mdp_json_local}\"):\n",
    "    mdp_json_paths_local.append(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd98d574-8644-4c81-984b-0f9acb721cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1623c1d9-9257-44a0-a2aa-920a30a8d974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "for fpath in tqdm(mdp_json_paths_local[:2], desc=f\"Loading JSON files: {fpath}\"):\n",
    "    fpath = re.sub(\"dbfs:\", \"/dbfs\", fpath) # Convert to Databricks path\n",
    "    with open(fpath, 'r') as fi:\n",
    "        data = json.load(fi)\n",
    "    if 'playlists' in data:\n",
    "        playlists = data['playlists']\n",
    "        df_playlists = spark.createDataFrame(playlists).coalesce(1)\n",
    "        fname_parts = os.path.basename(fpath).split(\".\")[:-1]\n",
    "        fname_parts.append('parquet')\n",
    "        fname = '.'.join(fname_parts)\n",
    "        df_playlists.write.mode(\"overwrite\").save(f\"{adls_mnt_path}/{mpd_dataset}/{fname}\")\n",
    "    else:\n",
    "        playlist = []\n",
    "        logger.error(f\"No playlists in {fpath}\")\n",
    "\n",
    "display(df_playlists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe36efc-1335-4f4f-aed9-4370503b2779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test = spark.read.parquet(f\"{adls_mnt_path}/{mpd_dataset}/{fname}\")\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fefb902-c382-44a1-8fd7-72138b5e72da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4ae403-6ad3-4a7b-bd0b-ffcd2441bc85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['playlists'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42593cb1-b330-4657-9989-da57ecafbe54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load MDP. About 6 minutes\n",
    "df = spark.read.option(\"multiline\", \"true\").json(mdp_json_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9e41229-69d8-4461-99e6-604aa7b756e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "```\n",
    "+--------------------+--------------------+\n",
    "|                info|           playlists|\n",
    "+--------------------+--------------------+\n",
    "|{2017-12-03 08:41...|[{false, NULL, 11...|\n",
    "+--------------------+--------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49be0607-80ed-49b2-9ada-3fe16db8ac47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df[[\"playlists\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47e2d69e-007c-42b8-b03d-a3d3b40f311e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "An example playlist entry.\n",
    "\n",
    "```\n",
    "{\n",
    "        \"name\": \"musical\",\n",
    "        \"collaborative\": \"false\",\n",
    "        \"pid\": 5,\n",
    "        \"modified_at\": 1493424000,\n",
    "        \"num_albums\": 7,\n",
    "        \"num_tracks\": 12,\n",
    "        \"num_followers\": 1,\n",
    "        \"num_edits\": 2,\n",
    "        \"duration_ms\": 2657366,\n",
    "        \"num_artists\": 6,\n",
    "        \"tracks\": [\n",
    "            {\n",
    "                \"pos\": 0,\n",
    "                \"artist_name\": \"Degiheugi\",\n",
    "                \"track_uri\": \"spotify:track:7vqa3sDmtEaVJ2gcvxtRID\",\n",
    "                \"artist_uri\": \"spotify:artist:3V2paBXEoZIAhfZRJmo2jL\",\n",
    "                \"track_name\": \"Finalement\",\n",
    "                \"album_uri\": \"spotify:album:2KrRMJ9z7Xjoz1Az4O6UML\",\n",
    "                \"duration_ms\": 166264,\n",
    "                \"album_name\": \"Dancing Chords and Fireflies\"\n",
    "            },\n",
    "            {\n",
    "                \"pos\": 1,\n",
    "                \"artist_name\": \"Degiheugi\",\n",
    "                \"track_uri\": \"spotify:track:23EOmJivOZ88WJPUbIPjh6\",\n",
    "                \"artist_uri\": \"spotify:artist:3V2paBXEoZIAhfZRJmo2jL\",\n",
    "                \"track_name\": \"Betty\",\n",
    "                \"album_uri\": \"spotify:album:3lUSlvjUoHNA8IkNTqURqd\",\n",
    "                \"duration_ms\": 235534,\n",
    "                \"album_name\": \"Endless Smile\"\n",
    "            },\n",
    "            {\n",
    "                \"pos\": 2,\n",
    "                \"artist_name\": \"Degiheugi\",\n",
    "                \"track_uri\": \"spotify:track:1vaffTCJxkyqeJY7zF9a55\",\n",
    "                \"artist_uri\": \"spotify:artist:3V2paBXEoZIAhfZRJmo2jL\",\n",
    "                \"track_name\": \"Some Beat in My Head\",\n",
    "                \"album_uri\": \"spotify:album:2KrRMJ9z7Xjoz1Az4O6UML\",\n",
    "                \"duration_ms\": 268050,\n",
    "                \"album_name\": \"Dancing Chords and Fireflies\"\n",
    "            },\n",
    "            // 8 tracks omitted\n",
    "            {\n",
    "                \"pos\": 11,\n",
    "                \"artist_name\": \"Mo' Horizons\",\n",
    "                \"track_uri\": \"spotify:track:7iwx00eBzeSSSy6xfESyWN\",\n",
    "                \"artist_uri\": \"spotify:artist:3tuX54dqgS8LsGUvNzgrpP\",\n",
    "                \"track_name\": \"Fever 99\\u00b0\",\n",
    "                \"album_uri\": \"spotify:album:2Fg1t2tyOSGWkVYHlFfXVf\",\n",
    "                \"duration_ms\": 364320,\n",
    "                \"album_name\": \"Come Touch The Sun\"\n",
    "            }\n",
    "        ],\n",
    "\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91fd1067-ca12-4171-a8b2-b0d376bd6252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mpd_data_process",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
