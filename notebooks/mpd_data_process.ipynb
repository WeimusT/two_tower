{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06eb1bf3-ab2e-48c7-9c40-6197a6f63382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20445884-4c57-4c2b-859c-7212795c56f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Config\n",
    "\n",
    "Prerequisite is to allow the Databrick environment to load from ADLS container. Follow this tutorial:\n",
    "\n",
    "[Tutorial: Connect to Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-gb/azure/databricks/connect/storage/tutorial-azure-storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae859618-e81a-433c-83b5-104b2bf85828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DBK_SECRET_SCOPE = \"tichack2024kv\" # Databricks secret scope to access Azure Key Vault\n",
    "AKV_KEY_NAME = \"analytical-databricks-key\" # Azure Key Vault\n",
    "ENTRA_APP_ID = \"9156dfe1-254b-4047-9f1a-a8fd3e79787d\"\n",
    "ENTRA_DIRECTORY_ID = \"565f1c8e-754e-473e-8352-ac5b86a38c93\" # Tenant ID of Entra App\n",
    "\n",
    "STORAGE_ACC = \"agenticaiamlws\" # Storage Account\n",
    "ADLS_CONTAINER = \"azureml-blobstore-03a975f6-17cd-4334-a581-d30d363b62ab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b93c279-5332-490b-b02c-7be2e702a6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_credential = dbutils.secrets.get(scope=DBK_SECRET_SCOPE, key=AKV_KEY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3a3633-ecb6-441a-b722-0ebc3bd8900d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mount ADLS Container and Unzip\n",
    "\n",
    "[Mounting cloud object storage on Azure Databricks](https://learn.microsoft.com/en-gb/azure/databricks/dbfs/mounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f30b44de-ca33-46db-97c2-9d812612b035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "          \"fs.azure.account.oauth2.client.id\": ENTRA_APP_ID,\n",
    "          \"fs.azure.account.oauth2.client.secret\": service_credential,\n",
    "          \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{ENTRA_DIRECTORY_ID}/oauth2/token\"}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=f\"abfss://{ADLS_CONTAINER}@{STORAGE_ACC}.dfs.core.windows.net\",\n",
    "    mount_point=\"/mnt/adls\",\n",
    "    extra_configs=configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d85710-07a6-4da4-a1e2-408d3aebc09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define Paths\n",
    "adls_mnt_path = \"/dbfs/mnt/adls\"\n",
    "\n",
    "# Fake path\n",
    "zip_adls_file = \"<fake/path>\" # Challenge path\n",
    "extract_path = \"<fake/path>\"\n",
    "\n",
    "# # Challenge\n",
    "# zip_adls_file = \"million_playlist_dataset/spotify_million_playlist_dataset_challenge.zip\" # Challenge path\n",
    "# extract_path = \"/dbfs/mnt/adls/challenge_dataset/\"\n",
    "\n",
    "# # MLD\n",
    "# zip_adls_file = \"million_playlist_dataset/spotify_million_playlist_dataset.zip\" # MPD path\n",
    "# extract_path = \"/dbfs/mnt/adls/mld_dataset/\"\n",
    "\n",
    "# Create extract folder\n",
    "zip_path = os.path.join(adls_mnt_path, zip_adls_file)  # Databricks paths use /dbfs/\n",
    "os.makedirs(extract_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f938dbb7-121e-4c6b-b01d-a881e34a691a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The extraction can take 30 minutes. Total extracted JSON size ~33GB and 1000 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4313b863-0644-4a2e-afac-a0b1d320cfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract ZIP file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "    zf.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a50f940-a19e-4583-bb29-a1d9c3f657e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Load JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad2f97c-bf93-4dfc-a96c-cba5cbf9c9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set spark connection\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{STORAGE_ACC}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{STORAGE_ACC}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{STORAGE_ACC}.dfs.core.windows.net\", ENTRA_APP_ID)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{STORAGE_ACC}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{STORAGE_ACC}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{ENTRA_DIRECTORY_ID}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a2f392c-b035-4a4c-8178-c725aa851e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test spark connection\n",
    "df_titanic = spark.read.csv(f\"abfss://{ADLS_CONTAINER}@{STORAGE_ACC}.dfs.core.windows.net/titanic.csv\", header=True)\n",
    "display(df_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da492dcd-d910-4a63-98b9-09ea8e87bbb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Million Playlist Dataset path\n",
    "mpd_dataset = \"mld_dataset/data/\"\n",
    "mdp_adls_path = f\"abfss://{ADLS_CONTAINER}@{STORAGE_ACC}.dfs.core.windows.net/{mpd_dataset}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "955ba732-46f3-43a9-a886-e1a7fa175a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load MDP. About 6 minutes\n",
    "df = spark.read.option(\"multiline\", \"true\").json(mdp_adls_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91fd1067-ca12-4171-a8b2-b0d376bd6252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mpd_data_process",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
