{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a73f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69188ab",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Feature engineering underpins every two-tower recommender, since each tower must transform raw user/item signals into embeddings that maximize similarity for true matches and minimize it for non-matches. In practice that means curating heterogeneous feature types—categorical IDs (users, items, genres, locales), numerical stats (recency, dwell time, purchase counts), textual or image embeddings harvested from pretrained models, and contextual signals like device, time of day, or geo buckets. Two-tower losses rely entirely on these engineered vectors; weak or noisy features collapse the embedding space, while thoughtful encodings (e.g., hashing high-cardinality IDs, normalizing numerics, sharing semantic vocabularies) bring enough structure for the towers to learn. Because user and item towers are trained separately yet paired only through a dot product, the completeness, consistency, and scale of engineered features directly control retrieval quality—so investing in robust pipelines for sourcing, cleaning, scaling, and regularly updating these feature sets is critical for a performant recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea6ca7",
   "metadata": {},
   "source": [
    "## ID Features\n",
    "IDs including User and Item IDs can be converted to embeddings. Typical dimensions for IDs:\n",
    "* User ID -> 16 - 128 dims\n",
    "* Item ID -> 32 - 256 dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d5624",
   "metadata": {},
   "source": [
    "## Categorical Features\n",
    "Category features can be converted to embeddings. Common categorical features include category, brand, country, device_type, gender, seller, shop, etc. Example dimensions:\n",
    "* category: 8 - 32\n",
    "* brand: 8 - 32\n",
    "* country: 4 - 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c2725",
   "metadata": {},
   "source": [
    "## Numerical Features\n",
    "Numerical or ordinal features can be normalised. Common normalization include: min-max, Z-score, Log-transform for heavy-tailed features. After preprocessing, the numerical features can be concatenated with other feature embeddings and sent to the model.\n",
    "\n",
    "```\n",
    "# After preprocessing\n",
    "x = torch.cat([user_id_emb, category_emb, torch.tensor([norm_price])], dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8daac",
   "metadata": {},
   "source": [
    "## Multi-hot Features\n",
    "Some features are better coded as Multi-hot, such as long-term user preference. More example features of this type includes:\n",
    "* Categories the user clicked in the past 30 days\n",
    "* Brands the user purchased frequently\n",
    "* Tags associated with an item\n",
    "* Interests inferred from long-term behaviour\n",
    "\n",
    "These features can be converted to Embedding Bag.\n",
    "\n",
    "```\n",
    "# Define embedding bag\n",
    "self.user_hist_cat_emb = nn.EmbeddingBag(n_categories, d, mode=\"mean\")\n",
    "# Usage of embedding bag to create a behavioral embedding for the user\n",
    "hist_emb = self.user_hist_cat_emb(hist_category_ids, offsets)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b64f62",
   "metadata": {},
   "source": [
    "## Sequencial Features\n",
    "\n",
    "Many features require a sequence model to encode, such as test description or short-term user behaviours. User behaviours is dynamic. The last few clicks strongly influence short-term interests. Last few impressions help track current intent. Order matters (“running shoes → socks → sports watch”). While long-term user preferences can be modelled using multi-hot embedding, short-term bahaviours such as last 10 minutes clicked, last 20 queries, or last 50 video watches, require a sequence model. Common sequence model are \n",
    "* Transformers (most common today)\n",
    "    - capture order\n",
    "\t- capture dependencies between items\n",
    "\t- powerful for session-based signals\n",
    "\n",
    "* GRU/LSTM (cheaper, still used)\n",
    "    - excellent trade-off between cost and quality\n",
    "    - widely used in Amazon/Alibaba retrieval stacks\n",
    "\n",
    "* 1D CNN (fast approximation)\n",
    "    - surprising effective for local sequences\n",
    "\n",
    "Sequential features are often used in production retrieval systems to capture user intent drift and sudden preference shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: User History Encoder using GRU\n",
    "class UserHistoryEncoder(nn.Module):\n",
    "    def __init__(self, n_items, emb_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, history_item_ids):\n",
    "        # history_item_ids: [B, T]\n",
    "        x = self.item_emb(history_item_ids)  # [B, T, emb_dim]\n",
    "        _, h = self.gru(x)                   # h: [1, B, hidden_dim]\n",
    "        return h.squeeze(0)                  # [B, hidden_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3324de5",
   "metadata": {},
   "source": [
    "The output of the `UserHistoryEncoder` can be concatenated with other features.\n",
    "\n",
    "```\n",
    "user_vec = concat(\n",
    "    user_id_emb,\n",
    "    country_emb,\n",
    "    device_emb,\n",
    "    long_term_pooled_emb,\n",
    "    short_term_sequence_emb\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d32f48",
   "metadata": {},
   "source": [
    "## Text Tower / Text Embedding\n",
    "\n",
    "Text-based signals are dominant in many systems:\n",
    "* product title\n",
    "* product description\n",
    "* movie synopsis\n",
    "* document content\n",
    "* app category description\n",
    "* video captions\n",
    "\n",
    "The industry practice are:\n",
    "* DistilBERT is common\n",
    "* Tiny-BERT or MiniLM for speed\n",
    "* Sometimes use precomputed embeddings shared across towers\n",
    "\n",
    "There are two main approaches:\n",
    "\n",
    "| Approach | Description | Pros | Cons |\n",
    "| --- | --- | --- | --- |\n",
    "| Precompute text embeddings | Generate all item text embeddings offline, store them as item features, and have the ItemTower simply load them. | Fast inference; text encoder cost removed from serving graph. | No end-to-end training, so text features cannot adapt during tower training. |\n",
    "| Fine-tune text encoder | Embed a trainable text encoder directly inside the ItemTower to produce item embeddings on the fly. | Better retrieval quality; embeddings adapt to domain; text semantics align with user behavior. | Training becomes expensive; embedding updates force offline re-indexing of the item catalog. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af818495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wtai/anaconda3/envs/two_tower/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/wtai/anaconda3/envs/two_tower/lib/python3.12/site-packages/torch/_C.cpython-312-darwin.so, 0x0002): Symbol not found: __ZN4absl12lts_2025012712log_internal10LogMessagelsIiTnNSt3__19enable_ifIXntsr4absl16HasAbslStringifyIT_EE5valueEiE4typeELi0EEERS2_RKS6_\n  Referenced from: <F0CE594F-5059-3403-BEDE-CC2EF3170AD7> /Users/wtai/anaconda3/envs/two_tower/lib/libprotobuf.29.3.0.dylib\n  Expected in:     <621B4947-F73F-3962-8DDB-2484D6B77411> /Users/wtai/anaconda3/envs/two_tower/lib/libabsl_log_internal_message.2501.0.0.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example: text Encoder using simple averaging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistilBertModel\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mItemTextEncoder\u001b[39;00m(nn.Module):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/two_tower/lib/python3.12/site-packages/transformers/__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     is_pretty_midi_available,\n\u001b[32m     37\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/two_tower/lib/python3.12/site-packages/transformers/dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/two_tower/lib/python3.12/site-packages/transformers/utils/__init__.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     ClassAttrs,\n\u001b[32m     26\u001b[39m     ClassDocstring,\n\u001b[32m     27\u001b[39m     ImageProcessorArgs,\n\u001b[32m     28\u001b[39m     ModelArgs,\n\u001b[32m     29\u001b[39m     ModelOutputArgs,\n\u001b[32m     30\u001b[39m     auto_class_docstring,\n\u001b[32m     31\u001b[39m     auto_docstring,\n\u001b[32m     32\u001b[39m     get_args_doc_from_source,\n\u001b[32m     33\u001b[39m     parse_docstring,\n\u001b[32m     34\u001b[39m     set_min_indent,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/two_tower/lib/python3.12/site-packages/transformers/utils/auto_docstring.py:30\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     MODELS_TO_PIPELINE,\n\u001b[32m     26\u001b[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[32m     27\u001b[39m     PT_SAMPLE_DOCSTRINGS,\n\u001b[32m     28\u001b[39m     _prepare_output_docstrings,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[32m     33\u001b[39m PATH_TO_TRANSFORMERS = Path(\u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m).resolve() / \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m AUTODOC_FILES = [\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodeling_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_extractor_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/two_tower/lib/python3.12/site-packages/transformers/utils/generic.py:51\u001b[39m\n\u001b[32m     47\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# required for @can_return_tuple decorator to work with torchdynamo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_debugging_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_addition_debugger_context\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# vendored from distutils.util\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/two_tower/lib/python3.12/site-packages/torch/__init__.py:367\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    366\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    371\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: dlopen(/Users/wtai/anaconda3/envs/two_tower/lib/python3.12/site-packages/torch/_C.cpython-312-darwin.so, 0x0002): Symbol not found: __ZN4absl12lts_2025012712log_internal10LogMessagelsIiTnNSt3__19enable_ifIXntsr4absl16HasAbslStringifyIT_EE5valueEiE4typeELi0EEERS2_RKS6_\n  Referenced from: <F0CE594F-5059-3403-BEDE-CC2EF3170AD7> /Users/wtai/anaconda3/envs/two_tower/lib/libprotobuf.29.3.0.dylib\n  Expected in:     <621B4947-F73F-3962-8DDB-2484D6B77411> /Users/wtai/anaconda3/envs/two_tower/lib/libabsl_log_internal_message.2501.0.0.dylib"
     ]
    }
   ],
   "source": [
    "# Example: text Encoder using simple averaging\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "class ItemTextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.text_encoder(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state[:, 0, :]   # [CLS] embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878dee5",
   "metadata": {},
   "source": [
    "## Feature Interactions\n",
    "\n",
    "Deep & Wide tower. While MLP are the deep part, feature interactions are the wide part. Feature interactions can capture highly predictive interaction that MLP alone struggle to discovery. Feature interactions are the manually created crosses. Examples of wide crosses include:\n",
    "* country x device_type\n",
    "* category x price_bucket\n",
    "* brand x gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05023e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b913bd32",
   "metadata": {},
   "source": [
    "# Two-Towers Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 10000\n",
    "n_items = 50000\n",
    "n_countries = 50\n",
    "n_categories = 200\n",
    "emb_dim = 32 # embedding dimension per feature\n",
    "tower_dim = 64 # output dimension of each tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Define the two towers\n",
    "# ---------------------------\n",
    "class UserTower(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.country_emb = nn.Embedding(n_countries, emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, tower_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(tower_dim, tower_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, user_id, country_id):\n",
    "        \"\"\"\n",
    "        Perform forward pass of user tower.\n",
    "\n",
    "        Args:\n",
    "            user_id: Tensor of shape (batch_size,)\n",
    "            country_id: Tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        u_id = self.user_emb(user_id) # (batch_size, emb_dim)\n",
    "        u_country = self.country_emb(country_id) # (batch_size, emb_dim)\n",
    "        x = torch.cat([u_id, u_country], dim=-1) # (batch_size, emb_dim * 2)\n",
    "        out = self.mlp(x) # (batch_size, tower_dim)\n",
    "        # optional: L2-normalise\n",
    "        out = F.normalize(out, p=2, dim=-1)\n",
    "        return out\n",
    "\n",
    "class ItemTower(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "        self.cat_emb = nn.Embedding(n_categories, emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, tower_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(tower_dim, tower_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, item_id, category_id):\n",
    "        \"\"\"\n",
    "        Perform forward pass of item tower for the batch of items. The items contains both\n",
    "        positive and negative samples.\n",
    "\n",
    "        Args:\n",
    "            item_id: Tensor of shape (batch_size, N)\n",
    "            category_id: Tensor of shape (batch_size, N)\n",
    "        \"\"\"\n",
    "        i_id = self.item_emb(item_id) # (batch_size, N, emb_dim)\n",
    "        i_cat = self.cat_emb(category_id) # (batch_size, N, emb_dim)\n",
    "        x = torch.cat([i_id, i_cat], dim=-1) # (batch_size, N, emb_dim * 2)\n",
    "        out = self.mlp(x) # (batch_size, tower_dim)\n",
    "        # optional: L2-normalise\n",
    "        out = F.normalize(out, p=2, dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_tower = UserTower()\n",
    "        self.item_tower = ItemTower()\n",
    "    \n",
    "    def forward(self, user_id, country_id, item_id, category_id):\n",
    "        user_vec = self.user_tower(user_id, country_id) # (batch_size, tower_dim)\n",
    "        item_vec = self.item_tower(item_id, category_id) # (batch_size, N, tower_dim)\n",
    "        # Expand user_vec to match item_vec shape\n",
    "        user_exp_vec = user_vec.unsqueeze(1).expand(-1, item_vec.size(1), -1) # (batch_size, 1, tower_dim)\n",
    "        # Compute dot product similarity\n",
    "        logits = torch.sum(user_exp_vec * item_vec, dim=-1) # (batch_size,)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a73ba9",
   "metadata": {},
   "source": [
    "# Negative Sampling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling dataset\n",
    "class TwoTowerNegSamplingDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 interactions: Sequence[Tuple], # Sequence of (user_id, pos_item_id)\n",
    "                 user_to_hard_negs: Dict[int, List[int]],\n",
    "                 all_item_ids: Sequence[int], # Catalog of all item IDs\n",
    "                 num_hard_negs: int = 2, # number of hard negatives per positive\n",
    "                 num_random_negs: int = 2, # number of random negatives per positive\n",
    "                 user_to_pos_items: Dict[int, List[int]] = None # optional mapping of user to their positive items\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset for two-tower model with negative sampling (1 positive + hard negatives + random negatives).\n",
    "        Args:\n",
    "            interactions: List of (user_id, pos_item_id) tuples.\n",
    "            user_to_hard_negs: Dict mapping user_id to list of hard negative item_ids.\n",
    "            all_item_ids: Sequence of all item IDs in the catalog.\n",
    "            num_hard_negs: Number of hard negatives to sample per positive.\n",
    "            num_random_negs: Number of random negatives to sample per positive.\n",
    "            user_to_pos_items: Optional dict mapping user_id to their positive item_ids to avoid sampling positives as negatives.\n",
    "        \"\"\"\n",
    "        self.interactions = list(interactions)\n",
    "        self.user_to_hard_negs = user_to_hard_negs\n",
    "        self.all_item_ids = all_item_ids\n",
    "        self.num_hard_negs = num_hard_negs\n",
    "        self.num_random_negs = num_random_negs\n",
    "        self.user_to_pos_items = user_to_pos_items or self._build_user_pos_items()\n",
    "        # Convert all_item_ids to a set for faster lookup\n",
    "        self.all_item_id_set = set(all_item_ids)\n",
    "    \n",
    "    def _build_user_pos_items(self) -> Dict[int, List[int]]:\n",
    "        user_pos = {}\n",
    "        for u_id, i_id in self.interactions:\n",
    "            user_pos.setdefault(u_id, []).append(i_id)\n",
    "        return user_pos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.interactions)\n",
    "\n",
    "    def _sample_hard_negs(self, user_id: int, pos_item_id: int) -> List[int]:\n",
    "        hard_negs = self.user_to_hard_negs.get(user_id, [])\n",
    "        # Remove the positive items from hard negatives\n",
    "        filtered = [i for i in hard_negs if i != pos_item_id]\n",
    "        if not filtered:\n",
    "            return []\n",
    "        if len(filtered) <= self.num_hard_negs:\n",
    "            # If not enough hard negatives, return all available\n",
    "            return filtered\n",
    "        else:\n",
    "            # otherwise, randomly sample the required number\n",
    "            sampled = random.sample(filtered, min(self.num_hard_negs, len(filtered)))\n",
    "            return sampled\n",
    "    \n",
    "    def _sample_random_negs(self, user_id: int, pos_item_id: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Sample random negatives ensuring they are not in user's positive items.\n",
    "        - Randomly sample from all_item_ids until we have enough valid negatives.\n",
    "        - If a sampled item is already a positive for the user, drop it and sample again.\n",
    "        \"\"\"\n",
    "        user_pos_items = set(self.user_to_pos_items.get(user_id, [])) # Filter in-batch negatives\n",
    "        negatives = []\n",
    "        attempts = 0\n",
    "        max_attempts = self.num_random_negs * 10  # to avoid infinite loop\n",
    "        while len(negatives) < self.num_random_negs and attempts < max_attempts:\n",
    "            sampled_item = random.choice(self.all_item_ids)\n",
    "            if sampled_item != pos_item_id and sampled_item not in user_pos_items:\n",
    "                negatives.append(sampled_item)\n",
    "            attempts += 1\n",
    "        return negatives\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        For each interaction, return:\n",
    "        - one positive interaction (user_id, pos_item_id)\n",
    "        - num_hard_negs hard negative item_ids\n",
    "        - num_random_negs random negative item_ids\n",
    "\n",
    "        Returns:\n",
    "            user_id: int\n",
    "            pos_item_id: int\n",
    "        \"\"\"\n",
    "        user_id, pos_item_id = self.interactions[idx]\n",
    "\n",
    "        # 1) Positive\n",
    "        item_ids = [pos_item_id]\n",
    "        labels = [1.0]\n",
    "\n",
    "        # 2) Hard negatives\n",
    "        hard_negs = self._sample_hard_negs(user_id, pos_item_id)\n",
    "        item_ids.extend(hard_negs)\n",
    "        labels.extend([0.0] * len(hard_negs))\n",
    "\n",
    "        # 3) Random negatives\n",
    "        random_negs = self._sample_random_negs(user_id, pos_item_id)\n",
    "        item_ids.extend(random_negs)\n",
    "        labels.extend([0.0] * len(random_negs))\n",
    "\n",
    "        # 4) convert to tensors\n",
    "        user_id_tensor = torch.tensor(user_id, dtype=torch.long)\n",
    "        item_ids_tensor = torch.tensor(item_ids, dtype=torch.long)\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            'user_id': user_id_tensor, # Scalar tensor\n",
    "            'item_ids': item_ids_tensor, # Shape: (1 + num_hard_negs + num_random_negs,)\n",
    "            'labels': labels_tensor # Shape: (1 + num_hard_negs + num_random_negs,)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f555f74",
   "metadata": {},
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoTowerModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TwoTowerNegSamplingDataset(\n",
    "    interactions=[(0, 10), (1, 20), (2, 30)], # example interactions\n",
    "    user_to_hard_negs={\n",
    "        0: [11, 12, 13],\n",
    "        1: [21, 22, 23],\n",
    "        2: [31, 32, 33]\n",
    "    },\n",
    "    all_item_ids=list(range(n_items)),\n",
    "    num_hard_negs=2,\n",
    "    num_random_negs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14bc87",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "The training steps consist of a forward pass, a backward pass and the adjustment.\n",
    "\n",
    "The Binary Cross Entropy loss function is used here:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{BCE}}(y, \\hat{y})\n",
    "= - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* y = true label\n",
    "* $\\hat{y}$ = predicted probability\n",
    "* C = number of classes\n",
    "* $y_i \\in \\{0,1\\}$ is 1 for the correct class\n",
    "* $\\hat{y}_i$ = predicted probability for class i\n",
    "\n",
    "For a multiclass case it is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CE}}(y, \\hat{y})\n",
    "= - \\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07423bd1",
   "metadata": {},
   "source": [
    "Another option for loss function is contrastive loss. In contrastive learning, every other user's positive item in the batch becomes a negative automatically. This eliminate the need for explicit negative sampling for the loss. The contrastive loss drive the embeddings to be closer to the positive items and to be further from the negative items.\n",
    "\n",
    "For a batch of size B:\n",
    "* User embeddings: $u_1, u_2, ..., u_B$\n",
    "* Positive item embeddings: $v_1, v_2, ..., v_B$\n",
    "\n",
    "For user i, we want:\n",
    "* $u_i$ close to $v_i$ (positive)\n",
    "* $u_i$ far from $v_j$ where $j ≠ i$ (negatives)\n",
    "\n",
    "The loss for a single user is:\n",
    "\n",
    "$$\n",
    "L_i = - log ( \\frac{\\exp(u_i \\cdot v_i / \\tau)}{\\sum_j \\exp(u_i \\cdot v_j / \\tau)} )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec64950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(u, v, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss between user embeddings u and item embeddings v.\n",
    "\n",
    "    Args:\n",
    "        u: Tensor of shape (batch_size, tower_dim) - user embeddings\n",
    "        v: Tensor of shape (batch_size, tower_dim) - item embeddings (positive pairs)\n",
    "        temperature: scaling factor for logits\n",
    "    \"\"\"\n",
    "    # Normalise to unit sphere\n",
    "    u = F.normalize(u, p=2, dim=-1)\n",
    "    v = F.normalize(v, p=2, dim=-1)\n",
    "\n",
    "    # Similarity matrix: [batch_size, batch_size]\n",
    "    logits = u @ v.T / temperature\n",
    "\n",
    "    # labels: each row i has positive at position i\n",
    "    labels = torch.arange(len(u)).to(u.device)\n",
    "\n",
    "    # classic InfoNCE loss\n",
    "    return F.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c73139",
   "metadata": {},
   "source": [
    "## Training Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d15f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, shuffle=True) # DataLoader for batching\n",
    "\n",
    "for batch in loader:\n",
    "    user_ids = batch['user_id']          # (B,)\n",
    "    item_ids = batch['item_ids']         # (B, N)\n",
    "    labels = batch['labels']             # (B, N)\n",
    "\n",
    "    logits = model(\n",
    "        user_ids,\n",
    "        torch.zeros_like(user_ids), # dummy country_id\n",
    "        item_ids,\n",
    "        torch.zeros_like(item_ids)  # dummy category_id\n",
    "    )  # (B, N)\n",
    "    loss = loss_fn(logits.view(-1), labels.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward() # calculate gradients\n",
    "    optimizer.step() # update parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f7047",
   "metadata": {},
   "source": [
    "Many production models combine Contrastive loss (in-batch negatives, no sampler) + BCE loss (explicit negatives including impressions, category-level negatives). While contrastive loss gives global structure, BCE injects real negative signals based on impression. The total loss is written as \n",
    "\n",
    "$$\n",
    "loss = λ * contrastive_loss + (1 - λ) * BCE_loss\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5993712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387e1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08539a24",
   "metadata": {},
   "source": [
    "# Precompute Item Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # set model to eval mode\n",
    "user_tower = model.user_tower\n",
    "item_tower = model.item_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbfa42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids_all = torch.arange(n_items) # (n_items,)\n",
    "\n",
    "with torch.no_grad():\n",
    "    def compute_item_embeddings(item_ids, batch_size = 1024) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Precompute item embeddings for all items in the catalog.\n",
    "        Returns:\n",
    "            item_embeddings: Tensor of shape (n_items, batch_size, tower_dim)\n",
    "        \"\"\"\n",
    "        embs = []\n",
    "        for i in range(0, len(item_ids), batch_size):\n",
    "            batch = item_ids[i:i+batch_size]\n",
    "            # reshape to (batch_size, 1)\n",
    "            batch = batch.unsqueeze(1)\n",
    "            v = item_tower(batch)\n",
    "            v = v.squeeze(1) # (batch_size, tower_dim)\n",
    "            embs.append(v)\n",
    "        return torch.cat(embs, dim=0) # (n_items, tower_dim)\n",
    "    \n",
    "    item_embeddings = compute_item_embeddings(item_ids_all) # (n_items, tower_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044b5d9",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "Two-tower at inference: retrieval with precomputed item embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8724cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss # FAISS library for efficient similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd58677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index\n",
    "d = item_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(d) # inner product index\n",
    "index.add(item_embeddings.cpu().numpy()) # add item embeddings to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend for a given user\n",
    "def recommend_for_user(user_id: int, country_id: int, top_k: int = 10) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Recommend top-k items for the given user using FAISS index.\n",
    "    Args:\n",
    "        user_id: int\n",
    "        country_id: int\n",
    "        top_k: int\n",
    "    Returns:\n",
    "        List of (item_id, score) tuples\n",
    "    \"\"\"\n",
    "    user_id_tensor = torch.tensor([user_id], dtype=torch.long)\n",
    "    country_id_tensor = torch.tensor([country_id], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        user_vec = user_tower(user_id_tensor, country_id_tensor) # (1, tower_dim)\n",
    "        user_vec_np = user_vec.cpu().numpy().astype('float32') # FAISS requires float32\n",
    "        # Search in FAISS index\n",
    "        D, I = index.search(user_vec_np, top_k) # D: distances, I: indices\n",
    "        recommendations = [(int(item_id), float(score)) for item_id, score in zip(I[0], D[0])]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae6d67",
   "metadata": {},
   "source": [
    "# Offline Evaluation\n",
    "\n",
    "Common evaluation metrics for recommendation problems are:\n",
    "\n",
    "__Hit@K__: did the true item appear in the top-K?\n",
    "\n",
    "__Recall@K__: what fraction of the user's \"relevant\" items are in the top-K? Recall@K is defined as\n",
    "\n",
    "```Recall@K = (# true positives in top-K) / (# total true positives)```\n",
    "\n",
    "Recall@K is the primary metric for two-tower retrieval.\n",
    "\n",
    "For example,\n",
    "```\n",
    "Ground truth = {7, 10, 25}\n",
    "Ranking = [3, 7, 10, 2, 25, 14, ...]\n",
    "```\n",
    "\n",
    "then \n",
    "* Hit@3 → True (because 7 and 10 are in the top 3)\n",
    "* Recall@3 → 2/3 (the model retrieved 2 of the 3 relevant items)\n",
    "\n",
    "NDCG@K (Normalised Discounted Cumulative Gain) is a more nuanced ranking metric. \n",
    "* Top positions get high weight\n",
    "* Lower positions get discounted\n",
    "* Supports multiple positives\n",
    "\n",
    "Usually, it is defined as:\n",
    "\n",
    "NDCG@K = DCG / IDCG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52453d64",
   "metadata": {},
   "source": [
    "__MRR (Mean Reciprocal Rank)__ rewards placing the true item very high. \n",
    "\n",
    "```if the true item is at rank 2 --> MRR = 1/2```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0aed83",
   "metadata": {},
   "source": [
    "Instead of evaluating using all items in the catalog (often contains millions of items), the industry pattern is to use positive + sampled negatives to build the evaluation set. Here is the steps:\n",
    "1.\tPick positives for evaluation\n",
    "Items the user interacted with at test time (not in training).\n",
    "2.\tSample negatives\n",
    "* Often ~100 random items\n",
    "* or ~1000 items from similar category\n",
    "* Optionally: the same negatives used in training are avoided\n",
    "3.\tScore them\n",
    "Use:\n",
    "* user_tower(user_id)\n",
    "* item_tower(candidate_items)\n",
    "* dot product similarity\n",
    "4.\tRank\n",
    "Sort by score descending.\n",
    "5.\tCompute metrics\n",
    "* Hit@K\n",
    "* Recall@K\n",
    "* MRR (mean reciprocal rank)\n",
    "* NDCG@K (discounts position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "689ee14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_users, user_to_test_pos, all_items, K=10, num_neg = 100):\n",
    "    model.eval() # set model to eval mode\n",
    "    user_tower = model.user_tower\n",
    "    item_tower = model.item_tower\n",
    "\n",
    "    hit_count = 0\n",
    "    recall_count = 0\n",
    "    total_positives = 0\n",
    "\n",
    "    for user in test_users:\n",
    "        pos_items = user_to_test_pos[user]\n",
    "        total_positives += len(pos_items)\n",
    "\n",
    "        # 1) Sample negatives\n",
    "        neg_items = torch.tensor(\n",
    "            random.sample(all_items, num_neg),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # 2) Form candidate set = positives + negatives\n",
    "        candidate_items = torch.cat([\n",
    "            torch.tensor(pos_items, dtype=torch.long),\n",
    "            neg_items\n",
    "        ]) # (num_pos + num_neg,)\n",
    "\n",
    "        # 3) Score them\n",
    "        user_tensor = torch.tensor([user], dtype=torch.long)\n",
    "        u = user_tower(user_tensor) # (1, tower_dim)\n",
    "        v = item_tower(candidate_items.unsqueeze(1)).squeeze(1)  # (num_candidates, tower_dim)\n",
    "        scores = torch.matmul(v, u.squeeze(0)) # (num_candidates,)\n",
    "\n",
    "        # 4) Rank\n",
    "        topk_indices = torch.topk(scores, K).indices # (K,)\n",
    "        topk_items = candidate_items[topk_indices] # (K,)\n",
    "\n",
    "        # 5) Metrices\n",
    "        # Hit@K\n",
    "        if any(item in pos_items for item in topk_items):\n",
    "            hit_count += 1\n",
    "        \n",
    "        # Recall@K\n",
    "        recall_count += sum(1 for item in topk_items if item in pos_items)\n",
    "\n",
    "        return {\n",
    "            'Hit@K': hit_count / len(test_users),\n",
    "            'Recall@K': recall_count / total_positives\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef719886",
   "metadata": {},
   "source": [
    "# Ranking\n",
    "\n",
    "Two-tower similarity (dot-product / cosine) reflects:\n",
    "* how much a user embedding aligns with\n",
    "* an item embedding\n",
    "\n",
    "But it does not consider:\n",
    "* context\n",
    "* freshness\n",
    "* popularity bias\n",
    "* business constraints\n",
    "* short-term patterns\n",
    "* price sensitivity\n",
    "* real-time signals\n",
    "* item quality\n",
    "* clickability\n",
    "* image attractiveness\n",
    "* long descriptions\n",
    "* dwell‐time behavior\n",
    "* real conversion predictions\n",
    "\n",
    "So retrieval towers capture stable, long-term user–item compatibility, while re-rankers capture contextual, short-term, and business-critical factors. If the catalog rarely changes and context doesn’t matter, the re-ranker becomes less important. But for real recommender systems, re-ranking solves three fundamental limitations of retrieval towers.\n",
    "\n",
    "1. Retrieval Towers Are Optimized for Recall, Not Ranking, i.e. Pull relevant items into the top-K, not to perfectly order them inside top-K. Two-tower dot-product similarity is not expressive enough to order subtle differences between items.\n",
    "\n",
    "2. Two-Tower Models Cannot Use Rich Features like text features (BERT), images (ResNET/ViT), etc, but ranking models can, because they don’t require:This is what's said by ChatGPT, but I doubt if it is hallucinating because ranking model is the next step of retrieval model. Using two-tower model is to speed up the retrieval using ANN, when we also need a fast re-ranking model.\n",
    "\n",
    "3. Retrieval Embeddings Cannot Capture Business Rules. Examples:\n",
    "* “We must show sponsored items preferentially.”\n",
    "* “Diversity rules: show items from different categories.”\n",
    "* “Hide items out of stock.”\n",
    "* “Avoid repeating the same item 3 times.”\n",
    "* “Prioritize local sellers in this region.”\n",
    "* “Don’t show NSFW content.”\n",
    "* “Boost new items.”\n",
    "\n",
    "Two-tower retrieval cannot enforce these, but ranking can.\n",
    "\n",
    "4. Retrieval Optimizes a Different Objective. Retrieval training is usually:\n",
    "* contrastive loss\n",
    "* or BCE\n",
    "* or sampled softmax\n",
    "\n",
    "Ranking training is usually:\n",
    "* listwise loss\n",
    "* pairwise ranking loss (RankNet / LambdaRank)\n",
    "* click prediction loss\n",
    "* conversion prediction loss\n",
    "* CTR/CVR modeling\n",
    "\n",
    "Retrieval chooses “good candidates.” while ranking chooses “best order.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e58b8",
   "metadata": {},
   "source": [
    "# Practical Project\n",
    "\n",
    "https://www.kaggle.com/code/twtw5201/book-recommendation/edit\n",
    "\n",
    "Use Amazon's book recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e5981",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering\n",
    "\n",
    "These are the most powerful techniques used in large-scale retrieval systems.\n",
    "\n",
    "We’ll cover:\n",
    "1.\tShort-term + long-term dual-tower design\n",
    "2.\tHierarchical sequence encoders\n",
    "3.\tMulti-modal item towers (text + image + video)\n",
    "4.\tMixture-of-Experts (MoE) towers\n",
    "5.\tFeature gating / routing\n",
    "6.\tDistributional embeddings (TikTok/Facebook)\n",
    "7.\tDynamic user state modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f5def",
   "metadata": {},
   "source": [
    "__Short-Term + Long-Term User Representations__\n",
    "\n",
    "Almost all modern retrieval systems use two separate user representations. Long-term embedding captures \"who the user is\": stable interests, average category distributions, embeddings from months of interactions, often an embedding bag, averaged over large history. Short-term embedding captures \"what the user wants right now\": last session, last X clicks, most recent query and are often RNN/Transformer based. The final embeddingare concatenated and often gated.\n",
    "\n",
    "```\n",
    "gate = sigmoid(W * concat(u_long, u_short))\n",
    "u_final = gate * u_short + (1 - gate) * u_long\n",
    "```\n",
    "\n",
    "This lets the model adapt:\n",
    "* when user intent is stable → long-term dominates\n",
    "* when user intent shifts fast → short-term dominates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf611b5",
   "metadata": {},
   "source": [
    "__Hierarchical Sequence Encoder__\n",
    "\n",
    "Hierarchical sequence encoder is used when sequences are long and noisy. The general structure is:\n",
    "1.\tEncode short segments with a small GRU (local patterns)\n",
    "2.\tPool segment outputs\n",
    "3.\tFeed pooled outputs into a second GRU/Transformer (global patterns)\n",
    "\n",
    "This helps:\n",
    "* reduce noise\n",
    "* encode very long histories (hundreds of interactions)\n",
    "* capture both short and medium-term patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbe6a6",
   "metadata": {},
   "source": [
    "__Multi-modal item towers__\n",
    "\n",
    "Items often have:\n",
    "* text\n",
    "* images\n",
    "* videos\n",
    "* numeric metadata\n",
    "* reviews\n",
    "* seller info\n",
    "\n",
    "A real item tower combines all of them.\n",
    "\n",
    "```\n",
    "v = concat(\n",
    "    item_id_emb,\n",
    "    category_emb,\n",
    "    title_bert_emb,\n",
    "    image_cnn_emb,\n",
    "    price_norm,\n",
    "    brand_emb\n",
    ")\n",
    "v = MLP(v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39ab89",
   "metadata": {},
   "source": [
    "__Mixture-of-Experts (MoE)__\n",
    "\n",
    "MoE is used when userbase is diverse.\n",
    "\n",
    "```\n",
    "UserTower:\n",
    "    MoE 1 → sports-oriented expert\n",
    "    MoE 2 → fashion-oriented expert\n",
    "    MoE 3 → tech-oriented expert\n",
    "```\n",
    "\n",
    "Expert weights predicted by a gating network based on user features. This gives huge gains in:\n",
    "* personalization\n",
    "* niche categories\n",
    "* robustness to data sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1e2aa",
   "metadata": {},
   "source": [
    "__Feature Gating / Routing__\n",
    "\n",
    "Used when some features should dominate in certain contexts.\n",
    "\n",
    "Examples:\n",
    "\t•\tif user session is long → short-term tower has higher weight\n",
    "\t•\tif user comes from search → query embedding dominates\n",
    "\t•\tif country = US → price sensitivity dominates\n",
    "\t•\tif device = mobile → video preference dominates\n",
    "\n",
    "This makes embedding spaces context-aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bc1da",
   "metadata": {},
   "source": [
    "__Distributional Embeddings__\n",
    "\n",
    "Instead of representing a user as a single vector, represent as:\n",
    "•\tset of vectors\n",
    "•\tGaussian distribution\n",
    "•\tmixture of clusters\n",
    "•\tattention-weighted bag of embeddings\n",
    "\n",
    "Reasons:\n",
    "•\tuser preferences are multi-modal\n",
    "•\tsingle vector collapses too much information\n",
    "•\tmulti-vector retrieval improves coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab89bbb",
   "metadata": {},
   "source": [
    "__Dynamic User State Modeling__\n",
    "\n",
    "Real-time ER-type signals:\n",
    "•\tuser just clicked X\n",
    "•\tuser hovered over Y\n",
    "•\tuser scrolled to section Z\n",
    "•\tdwell time\n",
    "•\tsearch query edits\n",
    "\n",
    "A real two-tower system ingests real-time signals to form user embedding in milliseconds.\n",
    "\n",
    "This is the frontier of retrieval research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9932e0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59a580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "two_tower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
