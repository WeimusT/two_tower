{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4200454,"sourceType":"datasetVersion","datasetId":2476732}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"books_rating_path = \"/kaggle/input/amazon-books-reviews/Books_rating.csv\"\nbooks_data_path = \"/kaggle/input/amazon-books-reviews/books_data.csv\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"book_rating = pd.read_csv(books_rating_path, header=0)\nbook_rating","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"books_data = pd.read_csv(books_data_path, header=0)\nbooks_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing\n\nConvert original dataset to a clean dataset, such as missing value, get global lookup table.\n\n","metadata":{}},{"cell_type":"code","source":"# all_books = books_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train / Test Split\n\nThe train / test split needs to happen before negative sampling. Use leave-last-out for test set.","metadata":{}},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"# Create a dataset, a dataloader\nclass TwoTowersDataset(Dataset):\n    def __init__(self, \n                 df_interaction=None, df_catalog=None, \n                 interaction_csv_path=None, interaction_header=0,\n                 catalog_csv_path=None, catalog_header=0, \n                 random_negative_samples=0\n    ):\n        super().__init__()\n        self.random_negative_samples = random_negative_samples\n        \n        # Load interaction\n        if df_interaction is not None:\n            self.df_interaction = df_interaction\n        elif interaction_csv_path is not None:\n            self.df_interaction = pd.read_csv(interaction_csv_path, header=header)\n        \n        # Load catalog\n        if df_catalog is not None:\n            self.df_catalog = df_catalog\n        elif catalog_csv_path is not None:\n            self.df_catalog = pd.read_csv(catalog_csv_path, header=header)\n\n        # Preprocessing\n        self.df_catalog[\"bookId\"] = self.df_catalog.infoLink.str.extract(r\"id=([^&]+)\")\n        self.df_catalog.Title = self.df_catalog.Title.fillna(\"\")\n        \n        # Fillna\n        self.df_interaction.fillna({\"User_id\": \"00000000000000\"}, inplace=True) # Use '00000000000000' for unknown User_id\n        self.df_interaction.fillna({\"Id\": \"0000000000\"}, inplace=True) # Use '0000000000' for unknown book ID\n        \n        # Item ID lookup\n        raw_book_ids = sorted(list(set(self.df_catalog[\"Title\"]))) # sort raw ID for reproduceability\n        self.books = [raw_book_id for idx, raw_book_id in enumerate(raw_book_ids)]\n        self.book2idx = {raw_book_id:idx for idx, raw_book_id in enumerate(raw_book_ids)}\n        self.idx2book = {idx:raw_book_id for idx, raw_book_id in enumerate(raw_book_ids)}\n        \n        # User ID lookup\n        raw_user_ids = sorted(list(set(self.df_interaction.User_id))) # sort raw ID for reproduceability\n        self.users = [raw_user_id for raw_user_id in enumerate(raw_user_ids)]\n        self.user2idx = {raw_user_id:idx for idx, raw_user_id in enumerate(raw_user_ids)}\n        self.idx2user = {idx:raw_user_id for idx, raw_user_id in enumerate(raw_user_ids)}\n\n        # Set bookId as index of catalog\n        self.df_catalog.set_index(\"bookId\", inplace=True)\n        \n    def __len__(self):\n        return len(self.df_interaction)\n\n    def __getitem__(self, idx):\n        \"\"\"Get all interactions from a user.\n\n        Args\n        idx: interaction\n        \"\"\"\n        # Get positive interaction\n        pos_interaction = self.df_interaction.loc[idx, :]\n        user_id = pos_interaction.User_id\n        pos_item_id = pos_interaction.Title\n        all_pos_items = set(self.df_interaction[self.df_interaction.User_id == user_id].Title)\n        # Random sample for negatives\n        max_attempt = 1e3 # maximum attempt 1000 times\n        all_neg_items = np.array(list(set(self.books) - all_pos_items))\n        neg_sample_items = all_neg_items[np.random.randint(0, len(all_neg_items), self.random_negative_samples)]\n        # Return data item\n        return {\n            \"user_ids\": torch.tensor(self.user2idx.get(user_id, 0), dtype=torch.long), # 0 is unknown user ID\n            \"item_ids\": torch.tensor([self.book2idx.get(pos_item_id, 0)] + [self.book2idx.get(neg_item_id, 0) for neg_item_id in neg_sample_items], dtype=torch.long),\n            \"binary_scores\": torch.tensor([1] + [0 for neg_item_id in neg_sample_items], dtype=torch.float) \n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"books_rating_ds = TwoTowersDataset(\n    df_interaction=book_rating, \n    df_catalog=books_data,\n    random_negative_samples = 10\n)\n\nbooks_rating_ds[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Two Towers Model","metadata":{}},{"cell_type":"markdown","source":"## User Tower","metadata":{}},{"cell_type":"code","source":"# Create user tower\nclass UserTower(nn.Module):\n    def __init__(self, num_users, id_emb_dim, tower_emb_dim):\n        \"\"\"\n        User tower that converts user features into a user embedding for dot product.\n\n        Args:\n        num_users - total users\n        id_emb_dim - Dimension of user ID embedding\n        tower_emb_dim - Dimension of user tower embedding\n        \"\"\"\n        super().__init__()\n        self.id_emb = nn.Embedding(num_users, id_emb_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(id_emb_dim, tower_emb_dim),\n            nn.ReLU(),\n            nn.Linear(tower_emb_dim, tower_emb_dim)\n        )\n\n    def forward(self, user_id):\n        x = self.id_emb(user_id) # [B, id_emb_dim]\n        x = self.mlp(x)\n        # Apply L2 normalization so to enable cosine similarity using x instead of unbounded dot product\n        x = x / x.norm(dim=-1, keepdim=True)\n        return x\n\n\nuser_id_emb_dim = 32\nuser_tower_emb_dim = 32\nnum_users = len(books_rating_ds.user2idx)\n\nuser_tower = UserTower(num_users, user_id_emb_dim, user_tower_emb_dim)\nuser_tower","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Item Tower","metadata":{}},{"cell_type":"code","source":"# Create item tower\nclass ItemTower(nn.Module):\n    def __init__(self, num_items, id_emb_dim, tower_emb_dim):\n        super().__init__()\n        self.id_emb = nn.Embedding(num_items, id_emb_dim) # [num_items, id_emb_dim]\n        self.mlp = nn.Sequential(\n            nn.Linear(id_emb_dim, tower_emb_dim),\n            nn.ReLU(),\n            nn.Linear(tower_emb_dim, tower_emb_dim)\n        )\n\n    def forward(self, item_id):\n        \"\"\"Calculate item tower embedding.\n\n        Args:\n        item_id - Item ID\n        \n        Returns:\n        \"\"\"\n        x = self.id_emb(item_id)  # [B, id_emb_dim]\n        x = self.mlp(x) # [B, tower_emb_dim]\n        x = x / x.norm(dim = -1, keepdim=True) # [B, tower_emb_dim]\n        return x\n\nnum_items = len(books_rating_ds.book2idx)\nid_emb_dim = 32\ntower_emb_dim = 32\n\nitem_tower = ItemTower(num_items, id_emb_dim, tower_emb_dim)\nitem_tower","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Two-towers model","metadata":{}},{"cell_type":"code","source":"# Create two-towers model\n\nclass TwoTowersModel(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        # Item tower dimension [item_id_emb_dim, item_tower_emb_dim]\n        self.item_tower = ItemTower(\n            num_items = kwargs.get(\"num_items\", 0),\n            id_emb_dim = kwargs.get(\"item_id_emb_dim\", 32),\n            tower_emb_dim = kwargs.get(\"item_tower_emb_dim\", 32),\n        )\n        # User tower dimension [user_id_emb_dim, user_tower_emb_dim]\n        self.user_tower = UserTower(\n            num_users = kwargs.get(\"num_users\", 0),\n            id_emb_dim = kwargs.get(\"user_id_emb_dim\", 32),\n            tower_emb_dim = kwargs.get(\"user_tower_emb_dim\", 32),\n        )\n\n    def forward(self, user_id, item_id):\n        \"\"\"Because item_emb and user_emb are normalised in the tower. The dot product here\n        is then the cosine similarity, and its value is between [-1, 1].\n\n        Args\n        user_id - User IDs, [B,]\n        item_id - Item IDs, [B,]\n        \"\"\"\n        item_emb = self.item_tower(item_id) # [B, item_tower_dim]. Normalised into Cosine similarity \n        user_emb = self.user_tower(user_id) # [B, user_tower_dim]. Normalised into Cosine similarity\n        x = (item_emb * user_emb).sum(axis=-1, keepdim=True).squeeze() # [B]. x elements are between -1 and 1.\n        x = nn.Sigmoid()(x) # Output in [0, 1] for BCE\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_item_ids = torch.tensor([0], dtype=torch.long)\ntest_user_ids = torch.tensor([0], dtype=torch.long)\n\ntest_model = TwoTowersModel(\n    num_items=len(books_rating_ds.books), \n    num_users=len(books_rating_ds.users)\n)\ntest_model(user_id=test_user_ids, item_id=test_item_ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Train\n\nTrain the two-towers model and monitor the loss and training epoch\n* Start with Binary Cross Entropy loss\n* Switch to Negative Contrastive Ex loss","metadata":{}},{"cell_type":"code","source":"from time import time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCH = 1\nBATCH_SIZE = 32\nMAX_BATCHES = None\nLOG_INTERVAL = 10\n\ndataloader = DataLoader(books_rating_ds, batch_size=BATCH_SIZE, shuffle=True)\nmodel = TwoTowersModel(    \n    num_items=len(books_rating_ds.books),\n    num_users=len(books_rating_ds.users)\n)\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a in-batch negative dataset that generates negative training samples from other user's positive items\n# The negative sampling dataset needs to be created for training and test individually\n\nfor epoch in range(EPOCH):\n    for i, batch in enumerate(dataloader):\n        batch_start_time = time()\n        if MAX_BATCHES is not None and i >= MAX_BATCHES:\n            break\n        \n        # -------------------------------------------------------\n        # Training\n        # -------------------------------------------------------\n        # Prepare Two-towers input\n        item_ids = batch[\"item_ids\"] # [B, K+1], K is number of negative samples, 1 is positive interaction\n        labels = batch[\"binary_scores\"] # [B, K+1]\n        user_ids = batch[\"user_ids\"] # [B, ]\n        user_ids_exp = user_ids.unsqueeze(1).expand(-1, item_ids.shape[1]) # [B, K+1]\n        # Flatten \n        user_ids_flat = user_ids_exp.reshape(-1) # [B*(K+1),]\n        item_ids_flat = item_ids.reshape(-1) # [B*(K+1),]\n        labels_flat = labels.reshape(-1) # [B*(K+1),]\n        # Forward Pass\n        logit = model(user_ids_flat, item_ids_flat) # forward pass\n        loss = loss_fn(logit, labels_flat)\n        # Backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        batch_finish_time = time()\n        elapsed_time = batch_finish_time - batch_start_time\n        \n        # -------------------------------------------------------\n        # Monitoring\n        # -------------------------------------------------------\n        if (i+1) % LOG_INTERVAL == 0:\n            print(f\"epoch {epoch+1}, batch {i+1}, \"\n                  f\"train loss: {loss.item()}, \"\n                  f\"user_ids: {user_ids.shape}, \"\n                  f\"item_ids: {item_ids.shape}, \"\n                  f\"labels: {labels.shape}, \"\n                  f\"elapsed time: {elapsed_time:.1f}s\"\n            )\n        ","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate offline performance metrics on the train and test set\n# Hit@K\n# Recall@K\n# Normalised Weighted Cumulative Gain@K\n# Mean Reciprocal Rank@K","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}